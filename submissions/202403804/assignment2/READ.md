# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 96.88%
- 훈련 시간: 1분 7초

## 실험 결과
### 실험 1: [실험명] 하이퍼파라미터튜닝
- 변경사항1: 에포크를 1->5->7->10 증가시켰을때 
- 결과: 에포크가 1회일 때 95.09%로 가장 높은 테스트 정확도를 보였고 에포크를 늘릴수록 테스트 정확도는 전반적으로 하락하는 경향을 보였다.
- 분석 :추가적인 훈련이 과적합 상태를 유발하여 오히려 일반화 성능을 낮출 수 있다는 것을 알게되었다. 

-  변경사항2: 학습률을 1e-1->1e-2->1e-4->1e-5 감소시켰을때 
- 결과: 36.55 -> 95.27 -> 89.86 -> 68.39 으로 학습률이 1e-2일때 가장 정확했고, 학습률이 커지거나 작아질수록 정확도가 낮았다.
- 분석: 학습률이 모델에 성능에 가장 큰 영향을 주는 요소였고, 1e-2일 때 모델의 최적의 균형값을 찾았다. 

### 실험 2: [실험명] 모델구조 개선 
- 변경사항: C. 은닉층 크기 및 깊이 
    {'name': 'C1', 'lr': 1e-3, 'epochs': 3, 'dropout': 0.0, 'hidden_dims': [50], 'activation_fn': nn.ReLU},
    {'name': 'C2', 'lr': 1e-3, 'epochs': 3, 'dropout': 0.0, 'hidden_dims': [200], 'activation_fn': nn.ReLU},
    {'name': 'C3', 'lr': 1e-3, 'epochs': 3, 'dropout': 0.0, 'hidden_dims': [100,100], 'activation_fn': nn.ReLU},
    {'name': 'C4', 'lr': 1e-3, 'epochs': 3, 'dropout': 0.0, 'hidden_dims': [100,100], 'activation_fn': nn.Tanh}
- 결과: 은닉층이 50일 때보다 200일때 성능이 가장 좋았고,  두 개의 은닉층을 사용한 C3,C4는 C2보다 성능이 낮았다. 그리고 은닉층이 2층일 때 nn.Tanh함수를 사용한 C4 성능이 C3다 좋았다.  
- 분석: 더 깊은 구조를 적용하는 것이 반드시 성능 향상을 보장하지 않았다.

-  변경사항:  D. 활성화 함수, Dropout 
    {'name': 'D1', 'lr': 1e-3, 'epochs': 3, 'dropout': 0.0, 'hidden_dims': [100], 'activation_fn': nn.Tanh},
    {'name': 'D2', 'lr': 1e-3, 'epochs': 3, 'dropout': 0.0, 'hidden_dims': [100], 'activation_fn': nn.ReLU},
    {'name': 'D3', 'lr': 1e-3, 'epochs': 5, 'dropout': 0.5, 'hidden_dims': [100], 'activation_fn': nn.Tanh},
    {'name': 'D4', 'lr': 1e-3, 'epochs': 5, 'dropout': 0.5, 'hidden_dims': [100], 'activation_fn': nn.ReLU}
- 결과:Dropout이 없을때가 Dropout이 있을때보다 더 좋은 성능을 보였고, Dropout (0.5)을 적용했을 때는 Tanh보다 ReLU의 테스트 정확도가 더 높았다. D3보다 D4가 정확했지만 훈련 정확도와 테스트 정확도 간의 차이가 더 컸다.
- 분석:ReLU가 Tanh보다 더 좋은 성능을 보이는 경향이 있었지만 Dropout(0.5)이 강한 규제로 작용하여 훈련 정확도를 크게 낮추고 오히려 ReLU 모델에서 일반화 성능의 격차를 키운 것 같다.
  
## 결론 및 인사이트
- 가장 효과적인 개선 방법: 학습률을 0.001로 설정했을때 다른 어떤 변경보다 가장 큰 성능 향상 효과를 보였다.
- 관찰된 패턴: 모델은 에포크3 이내에 충분히 정확도가 높았고, 에포크를 늘리는 것이 성능을 오히려 하락시켰다. 또, 단일 은닉층 네트워크가 더 깊은 네트워크보다 높은 성능을 보였으며 ReLU가 Tanh보다 일반적으로 우수한 성능을 보였다.
- 추가 개선 아이디어: Dropout 0.5가 너무 강했으므로, 0.1∼0.3 수준으로 낮춰서 훈련 정확도의 손실을 최소화해볼 수 있다. 또, ReLU를 사용하고 Dropout을 낮춘 상태에서 2층 이상의 은닉층으로 다시 실험해볼 수 있다. 

