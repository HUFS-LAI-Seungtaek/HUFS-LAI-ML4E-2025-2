# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 97.61%
- 훈련 시간: 3분 10초

## 실험 결과
### 실험 1: [학습률 낮추고 에포크 수를 증가시킴]
- 변경사항: 
학습률 : 0.001 --> 0.0005
에포크 수 : 3 --> 13

- 결과:
최종 훈련 정확도: 97.16% --> 99.46%
최종 테스트 정확도 : 96.95% --> 97.77%

- 분석:
학습률을 0.001에서 0.0005로 낮춰 모델이 가중치를 더 세밀하게 조정하도록 하였고 에포크 수를 3회에서 13회로 늘려 데이터 학습 기회를 충분히 확보하였다. 그 결과, 훈련 시간이 6분으로 늘었지만 전체적인 정확도는 향상되었다. 즉, 모델이 훈련 데이터에 대한 학습을 거의 완벽히 수행하였고 새로운 데이터 (테스트 데이터)에 대한 일반화 성능 역시 좋아졌음을 확인할 수 있다.

### 실험 2: [은닉층 사이즈 늘림]
- 변경사항:
은닉층 개수 : 1개 --> 3개
은닉층 크기: 100 --> 150 
에포크 수 : 13 --> 5
학슴률 : 0.0005 --> 0.0009

- 결과: 전체적인 정확도는 조금 떨어졌으나 속도가 절반으로 줄었다.
훈련 정확도: 99.46% --> 98.42%
테스트 정확도 : 97.77% --> 97.61%
속도 : 6분 2초 --> 3분 10초

- 분석:
은닉층의 개수과 크기를 늘려 모델의 표현력을 향상시키고자 하였고 학습 시간을 줄이기 위해 학습률을 높이고 에포크 수를 감소시켰다. 그 결과, 전체적인 정확도는 소폭 하락하였지만 훈련 시간은 절반으로 단축되었다. 즉, 시간 대비 효율적인 학습 성능을 보였다.
 

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 정확도 수치로만 보면 첫 번째 실험이 가장 효과적이었다. 그러나 그에 비해 훈련 시간이 지나치게 길어지는 한계가 있었다. 반면 두 번 째 실험은 정확도는 다소 낮았지만 훈련 시간이 절반 이하로 단축되어 시간 대비 효율적인 결과를 보여주었다. 

- 관찰된 패턴: 
은닉층을 확장한 모델은 훈련 정확도가 약간 낮아졌지만 테스트 정확도는 거의 유지되어 훈련 데이터에 과도하게 의존하기 않고 더 균형 잡힌 학습이 이러우진 것으로 보인다. 이는 과적합 완화되고 일반화 경향이 강화된 것으로 해석할 수 있다.

반면 학습률을 낮추고 에포크를 늘린 모델은 훈련 데이터에 과도하게 적응하는 과적합 가능성을 약간 나타냈다. 

따라서 은닉층 조정은 시간 효율성뿐 아니라 안정적인 학습 패턴 측면에서도 더 우수한 모델로 평가된다.

- 추가 개선 아이디어:
cpu 대신 gpu를 사용하여 학습 속도 향상