1. 프로젝트 개요

1.1 프로젝트 주제

국제/정치 뉴스 요약 봇 (Global & Politics News Summarizer)

1.2 목표

매일 발생하는 방대한 양의 국제 정세 및 정치 관련 뉴스를 수집하고, 핵심 내용을 '3줄 요약' 형태로 자동 변환하여 사용자에게 제공하는 AI 서비스 개발.

1.3 필요성

정보 과잉 시대에 사용자가 긴 기사를 모두 읽지 않고도 주요 정치/외교 이슈를 신속하게 파악할 수 있도록 돕기 위함.

2. 데이터 수집 (Data Collection)

본 프로젝트는 초기 웹 크롤링 방식의 불안정성(접속 차단, 페이지 구조 변경 등)을 해결하고, 지도 학습(Supervised Learning)을 위한 고품질의 정답 데이터(Label)를 확보하기 위해 오픈소스 데이터셋 활용으로 수집 전략을 변경했습니다.

수집 방법: Hugging Face datasets 라이브러리를 활용한 API 데이터 로드

데이터 소스: daekeun-ml/naver-news-summarization-ko

네이버 뉴스 기사 원문(document)과 사람이 직접 검수한 요약문(summary)이 쌍으로 구성된 데이터셋입니다.

데이터 구조:

title: 기사 제목

document: 기사 본문 (Model Input)

summary: 기사 요약문 (Model Output / Label)



3. 데이터 분석 및 주요 발견

수집된 22,194건의 데이터에 대한 기초 통계 및 시각화 분석 결과입니다.

3.1 기초 통계 

데이터 크기: 총 22,194건 (Training Set 기준)

텍스트 길이 분포:

본문: 평균 길이 약 998자

요약문: 평균 길이 약 183자

분석 결과:

대부분의 기사가 500~1,500자 사이에 분포하고 있습니다.

이는 BERT나 BART와 같은 LLM의 일반적인 입력 허용량 내에서 정보 손실(Truncation) 없이 충분히 학습이 가능한 수준입니다.


3.2 주요 발견 사항 

1) 심각한 도메인 편향 (Domain Bias)

현상: 키워드 매칭 분석 결과, '정치', '국회', '대통령' 등 정치 관련 핵심 단어가 제목에 포함된 기사는 **단 133건 (전체의 0.6%)**에 불과했습니다.

문제점: 데이터셋의 대부분이 경제, IT, 생활문화 뉴스로 편중되어 있습니다. 현재 데이터만으로는 국제 정세나 정치적 맥락(예: 외교 프로토콜, 법안 발의 과정 등)을 학습하기에 데이터가 턱없이 부족하다는 한계를 발견했습니다.

2) 데이터 품질 및 이상치 (Quality & Outliers)

현상 (역전 현상): 요약문의 길이가 본문보다 긴 논리적 오류 데이터가 1,739건 (약 7.8%) 발견되었습니다.

문제점: 이를 전처리 없이 학습할 경우, 모델이 내용을 요약하지 않고 오히려 생성해내거나 원문을 그대로 복사하는 환각(Hallucination) 현상을 유발할 수 있습니다.

4. 향후 계획 (Future Plan: Assignment 5)

EDA를 통해 파악된 '정치 데이터 부재' 문제와 '이상치'를 해결하기 위해 다음과 같은 구체적인 모델링 전략을 수립했습니다.

4.1 데이터 확보 전략 (Data Augmentation)

대체 데이터셋 도입: csebuetnlp/xlsum (Korean) 데이터셋을 추가로 도입합니다.

이는 전 세계적으로 신뢰도 높은 BBC News를 기반으로 구축된 다국어 요약 데이터셋입니다.

BBC의 특성상 국제 정세 및 정치/외교 기사가 풍부하여, 기존 데이터셋의 도메인 공백을 효과적으로 메울 수 있습니다.

실시간 파이프라인:

모델 추론(Inference) 단계에서는 네이버 뉴스 API를 활용해 실시간 '정치/세계' 뉴스를 수집하고, 이를 모델에 주입하는 자동화 파이프라인을 구축합니다.

4.2 모델 학습 전략 

전처리(Preprocessing):

요약문이 본문보다 긴 이상치(1,739건)와 50자 미만의 초단문 데이터를 학습 전 전량 **제거(Drop)**합니다.

전이 학습(Transfer Learning):

Stage 1 (General Ability): 기존 경제/IT 중심 데이터(2.2만 건)를 사용하여 모델에게 일반적인 '한국어 문장 압축 능력'을 학습시킵니다.

Stage 2 (Domain Adaptation): 확보될 BBC 기반 정치 뉴스(xlsum)를 사용하여 정치/외교 도메인에 특화된 **미세 조정(Fine-tuning)**을 수행합니다.
