# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 96.67%
- 훈련 시간: 60.59초

## 실험 결과
### 실험 1: [은닉층 변경]
- 변경사항: 기존 은닉층(100개)와 더불어 3번 더 은닉층을 변경하고, 각각 10번 반복하여 정확도와 표준편차 측정
- 결과: 은닉층 크기가 증가할수록 정확도가 점진적으로 상승하며, 표준편차는 전체적으로 낮은 수준(0.1~0.16)으로 안정적이다. 
- 분석: 작은 은닉층(50)에서는 표현력이 부족하여 정확도가 낮았으나, 100~400으로 커질수록 비선형 표현이 강화되어 정확도가 개선되었다. 다만, 증가 폭은 점점 완만해져서 200 이상부터는 성능 향상이 미미한 것을 확인할 수 있었다. seed를 고정하지 않았음에도 ±0.1~0.16 수준으로 편차가 작았다. 이는 모델 구조가 비교적 단순했기 때문으로 판단된다. 난수 고정을 하지 않음으로써 “훈련 과정의 불안정성”이 실험마다 반영되었으나 ‘은닉층이 커질수록 성능이 좋아진다’는 경향성은 유지되었다. 

### 실험 2: [활성화 함수에 따른 모델의 성능 파악]
- 변경사항: seed를 고정한 후, 다양한 활성화 함수를 적용하여 정확도 및 훈련시간을 산출하였다. 
- 결과: 모든 활성화 함수의 훈련 시간이 83초에서 87초 사이로 매우 근소한 차이를 보였으며, 이는 활성화 함수의 차이가 전체 훈련 시간에 미치는 영향이 크지 않음을 나타낸다. 또한 모든 활성화 함수가 MNIST 문제에서 매우 높은 성능(97% 이상)을 보였다. 특히 ELU가 미세하게 가장 우수했으며, 얕은 네트워크에서는 Tanh도 경쟁력이 있음을 확인하였다.
- 분석: 종합적으로 볼 때, MNIST 분류 문제와 단일 은닉층 MLP 환경에서는 활성화 함수에 따른 성능 차이가 매우 미미했다. 모든 함수가 최종적으로 0.04 ~ 0.05 사이의 낮은 손실로 수렴하여 학습이 잘 이루어졌음을 보여준다.

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 정확도를 개선하기 위해서는 은닉층을 늘리는 것이 중요하다. 그러나 은닉층을 과하게 늘릴 경우 과적합의 문제가 발생할 수 있으므로 적정한 수준에서 늘리는 것이 중요하다. 실제 딥러닝에서 모델과 데이터셋이 복잡해질수록 기울기 소실 문제가 발생하기에 이를 고려하여 적합한 활성화 함수를 선택해야 한다. 이런 점에 있어서는 LeakyReLU가 ReLU의 대안으로 사용될 수 있을 것이다.  
- 관찰된 패턴: 은닉층이 커질수록 성능이 좋아진다. 단일 은닉층 MLP 환경에서는 활성화 함수가 큰 영향을 미치지 않았다. 
- 추가 개선 아이디어: 은닉층, 활성화 함수와 더불어 학습률도 추세를 파악하며 변경한다면 더욱 개선된 성능의 모델을 얻을 수 있을 것이다. 
