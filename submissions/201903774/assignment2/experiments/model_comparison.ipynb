{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3d565",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#실험 적용 전 모델\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=589, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수 // forward propagation\n",
    "        x: 입력 텐서 (batch_size, 784)\n",
    "        return: 출력 텐서 (batch_size, 10)\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "# 모델 생성 및 구조 확인\n",
    "model = MLP()\n",
    "print(\"모델 구조:\")\n",
    "print(model)\n",
    "\n",
    "# 파라미터 개수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n총 파라미터 수: {total_params:,}\")\n",
    "print(f\"학습 가능한 파라미터 수: {trainable_params:,}\")\n",
    "\n",
    "# 각 레이어별 파라미터 수 확인\n",
    "print(\"\\n레이어별 파라미터:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} ({param.numel():,} 개)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335095a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#실험 적용 후 모델\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size1=589, hidden_size2=256, hidden_size3=128, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.BatchNorm1d(hidden_size1),   # normalize layer outputs\n",
    "            nn.LeakyReLU(),                 # activation\n",
    "            nn.Dropout(p=0.3),         # 30% dropout\n",
    "            \n",
    "            # Second hidden layer\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.BatchNorm1d(hidden_size2),\n",
    "            nn.Tanh(),                 # try tanh here\n",
    "            nn.Dropout(p=0.3),\n",
    "            \n",
    "            # Third hidden layer\n",
    "            nn.Linear(hidden_size2, hidden_size3),\n",
    "            nn.BatchNorm1d(hidden_size3),\n",
    "            nn.LeakyReLU(),                 # back to ReLU\n",
    "            nn.Dropout(p=0.3),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(hidden_size3, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수 // forward propagation\n",
    "        x: 입력 텐서 (batch_size, 784)\n",
    "        return: 출력 텐서 (batch_size, 10)\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "# 모델 생성 및 구조 확인\n",
    "model = MLP()\n",
    "print(\"모델 구조:\")\n",
    "print(model)\n",
    "\n",
    "# 파라미터 개수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n총 파라미터 수: {total_params:,}\")\n",
    "print(f\"학습 가능한 파라미터 수: {trainable_params:,}\")\n",
    "\n",
    "# 각 레이어별 파라미터 수 확인\n",
    "print(\"\\n레이어별 파라미터:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape} ({param.numel():,} 개)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
