# MNIST 분류 실험 결과

## 기본 모델 성능(10번 반복)
- 최종 테스트 정확도: 97.68% ± 0.18%
- 훈련 시간: 0분 46.71초 ± 0분 1.10초

## 실험 결과
### 실험 1: 하이퍼파라미터 튜닝
- 변경사항: 학습률 변경(1e-3 -> 5e-4 -> 1e-4 -> 5e-3 -> 1e-2 5번 반복), 배치 크기 변경(128 -> 118 -> 138 -> 148 -> 200, 학습률 1e-3으로 유지, 5번 반복), 에포크 수 변경(3 -> 2 -> 4 -> 5, 배치 크기 148로 증가, 학습률 1e-3으로 유지, 5번 반복), 은닉층 개수 변경(100 -> 500 -> 1000 -> 600 -> 589 -> 550, 에포크 수 4로 증가, 배치 크기 148로 증가, 학습률 1e-3으로 유지, 5번 반복)
- 결과: 최종 테스트 정확도의 평균 값 일부 감소 및 표준편차 일부 증가로 변동성 일부 증가(97.68% ± 0.18% -> 97.66% ± 0.27%), 훈련 시간 대폭 증가(0분 46.71초 ± 0분 1.10초 -> 1분 7.04초 ± 0분 2.36초)
- 분석: 모델의 정확도를 일관적이고 안정적으로 유지하기 위해선 학습률(1e-3)을 유지하되 배치 크기, 은닉층 개수, 에포크 수의 적절한 증가가 중요하다. 특히 에포크 수의 적절한 조정이 정확도 뿐만 아니라 시간 증가에도 영향을 끼치며, 모델의 정확도를 개선하기 위해선 해당 변수들의 적절한 조정이 필요하고 조정한 뒤에도 개선이 없다면 기본 모델을 그대로 활용하는 것이 더 적합할 수 있다.

### 실험 2: 모델 구조 개선
- 변경사항: #은닉층 2개 추가로 3층 신경망 구성 및 첫번째 은닉층 증가(개수: 2->3->4->3, hidden_size1=550 -> 589, hidden_size2=256, hidden_size3=128), LeakyReLU 및 Tanh 적용(전자는 첫 번째와 세 번째, 후자는 두 번째에 적용), Dropout 모든 층에 적용(p=0.3), BatchNormalization 1d 모든 층에 적용
- 결과: 최종 테스트 정확도가 기본 모델과 실험 1의 모델보다 유의미하게 증가(97.68% ± 0.18% -> 97.77% ± 0.31%), 훈련 시간 실험 1보다 유의미하게 감소(1분 7.04초 ± 0분 2.36초 -> 1분 2.64초 ± 0분 1.29초)
- 분석: 은닉층 수를 단순히 늘리는 것보다 활성함수 조합(LeakyReLU + Tanh)이 가장 결정적 향상 요인이며, Dropout과 BatchNorm은 성능보다는 안정화 측면에서 유의미하다.

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 실험 1에서 조정된 하이퍼파라미터를 통한 최적화(lr=1e-3, batch=148, epoch=4), 실험 2에서  LeakyReLU + Tanh 활성화 조합, BatchNorm1d 적용, 3층 네트워크 구조 (589→256→128), Dropout(0.3) 적용으로 각각 최고 정확도 향상, 학습 안정성 증가와 분산 감소, 깊이와 효율의 균형, 경량 과적합 방지를 부여한 것으로 관찰된다.
- 관찰된 패턴: 모델의 깊이는 표현력 향상에 기여하되, 복잡도 증가 시 일반화 성능 저하를 초래한다, 서로 다른 비선형 함수의 병용은 gradient 소실 방지 + 표현 다양성 확보에 긍정적이다, BatchNorm + Dropout 조합은 성능보다는 분산 안정화에 핵심 역할이다, 적절한 정규화·활성화 설계는 연산량 증가를 상쇄할 정도의 효율성을 제공한다.
- 추가 개선 아이디어: NAdam 최적하기, 가중치 초기화(Weight Initialization), 가변적 활성화(Flexible activation), 잔차 연결(Residual Connection)의 적용이 차후의 정확도 상승과 훈련 시간 감소를 위해 고려할 가치가 있다고 판단된다.