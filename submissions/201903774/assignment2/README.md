# MNIST 분류 실험 결과

## 기본 모델 성능
- 실험 진행 플랫폼: Google Colab, T4 GPU 연결
- 최종 테스트 정확도: 97.01%
- 훈련 시간: 0분 52초

## 실험 결과
### 실험 1: 하이퍼파라미터 튜닝
- 변경사항: 은닉층 크기 증가(hidden_size1: 100 -> 589), 배치 크기 증가(batch_size: 128 -> 138), 에포크 수 증가(nb_epochs: 3 -> 4)
- 결과: 최종 테스트 정확도 대폭 상승(97.01% -> 98.05%), 훈련 시간 증가(0분 47초 -> 1분 6초)
- 분석: 모델의 정확도 상승에는 은닉층 크기의 조정이 가장 큰 역할을 차지하고 그 다음으로 배치 크기와 에포크 수 조정이 영향을 미치며, 정확도가 상승할 수록 훈련 시간도 마찬가지로 정비례한다. 

### 실험 2: 모델 구조 개선
- 변경사항: 실험 1에서 튜닝한 하이퍼파라미터 유지, 은닉층 2개 추가로 3층 신경망 구성(hidden_size1=589, hidden_size2=256, hidden_size3=128), LeakyReLU 및 Tanh 적용(전자는 첫 번째와 세 번째, 후자는 두 번째에 적용), Dropout 모든 층에 적용(p=0.3), BatchNormalization 1d 모든 층에 적용, 최적하기 변경 (Adam -> NAdam)
- 결과: 최종 테스트 정확도 일부 상승 (97.01% -> 98.05% -> 98.10%), 훈련 시간 일부 감소 (0분 47초 -> 1분 6초 -> 1분 4초)
- 분석: 은닉층과 Dropout의 추가가 최종 테스트 정확도 상승과 안정화에 영향을 끼치고, 이외 ReLU 대신 LeakyReLU와 Tanh의 적용, BatchNormalization 정규화, NAdam을 적용한 최적하기 변경이 정확도 상승과 훈련 시간 감소에도 영향을 끼침을 확인하였다.

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 은닉층 크기와 개수의 적절한 조정, 다양한 활성 함수(Activation functions)와 Dropout 적용, BatchNormalization를 비롯한 추가 정규화의 도입과 최적하기 NAdam의 적용이 정확도 상승과 안정화에 가장 효과적인 것으로 드러났다.
- 관찰된 패턴: 실험 1와 실험 2 모두 4를 9로, 9를 8로 잘못 예측하는 패턴이 관찰되었으며, 그 이외 실험 1에선 4를 2로, 실험 2에선 2를 9로 잘못 예측하는 패턴이 발견되었다.
- 추가 개선 아이디어: 가중치 초기화(Weight Initialization), 가변적 활성화(Flexible activation), 잔차 연결(Residual Connection)의 적용이 차후의 정확도 상승과 훈련 시간 감소를 위해 고려할 가치가 있다고 판단된다.