# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 97.18%
- 훈련 시간: 0분 46초

## 실험 결과
### 실험 1: 하이퍼파라미터 튜닝
- 변경사항: 은닉층 크기 증가(hidden_size1: 100 -> 589), 배치 크기 증가(batch_size: 128 -> 138), 에포크 수 증가(nb_epochs: 3 -> 4)
- 결과: 최종 테스트 정확도 상승(97.06% -> 97.91%), 훈련 시간 증가(0분 47초 -> 1분)
- 분석: 모델의 정확도 상승에는 은닉층 크기의 조정이 가장 큰 역할을 차지하고 그 다음으로 배치 크기와 에포크 수 조정이 영향을 미치며, 정확도가 상승할 수록 훈련 시간도 마찬가지로 정비례한다. 

### 실험 2: 모델 구조 개선
- 변경사항: 은닉층 2개 추가로 3층 신경망 구성(hidden_size1=589, hidden_size2=256, hidden_size3=128), LeakyReLU 및 Tanh 적용(전자는 첫 번째와 세 번째, 후자는 두 번째에 적용), Dropout 모든 층에 적용(p=0.3), BatchNormalization 1d 모든 층에 적용
- 결과: 최종 테스트 정확도 상승 및 변동 안정화(97.91% -> 98.01%), 훈련 시간 변동 없음(1분)
- 분석: 은닉층과 Dropout의 추가가 최종 테스트 정확도 상승과 안정화에 영향을 끼치고, 특히 ReLU 대신 LeakyReLU와 Tanh를 적용하며 BatchNormalization 정규화를 통해 각각 증가와 안정화 효과가 있음을 보여준다.

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 은닉층 크기와 개수의 적절한 조정, 다양한 활성 함수(Activation functions)와 Dropout 적용, BatchNormalization을 비롯한 추가 정규화의 도입이 정확도 상승과 안정화에 가장 효과적인 것으로 드러났다.
- 관찰된 패턴: 숫자 3과 9를 8로, 숫자 2를 9로 오분류하는 패턴이 관찰되었다.
- 추가 개선 아이디어: 가중치 초기화(Weight Initialization), 가변적 활성화(Flexible activation), 잔차 연결(Residual Connection)의 적용도 정확도 상승을 위해선 고려할 가치가 있다고 판단된다.